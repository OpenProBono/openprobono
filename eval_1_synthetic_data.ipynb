{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenProBono RAG Evaluation, Part 1\n",
    "## Synthetic Data\n",
    "\n",
    "### About these notebooks\n",
    "\n",
    "These notebooks are based on [RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation) by [Aymeric Roucher](https://huggingface.co/m-ric). They have been split into 4 parts that build on top of each other.\n",
    "\n",
    "Any section inside block quotes is a direct quote. The general structure is copied and ideas are paraphrased. Prompts are adjusted to fit OpenProBono's use case. Code has been added and modified.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "These notebooks demonstrate how you can evaluate your RAG (Retrieval Augmented Generation) by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    ">For an introduction to RAG, you can check [this other cookbook](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain)!\n",
    ">\n",
    ">RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    ">\n",
    "><img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" alt=\"RAG workflow: Knowledge Base, Embedding Model, LLM, LLM Prompt, and more\" height=\"700\"/>\n",
    ">\n",
    ">Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system’s performance! So let’s see how to evaluate our RAG system.\n",
    ">\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    ">Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    ">\n",
    ">For our evaluation pipeline, we will need:\n",
    ">\n",
    ">1. An evaluation dataset with question - answer couples (QA couples)\n",
    ">2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    ">\n",
    ">➡️ It turns out, we can use LLMs to help us all along the way!\n",
    ">\n",
    ">1. The evaluation dataset will be synthetically generated by an LLM 🤖, and questions will be filtered out by other LLMs 🤖\n",
    ">2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent 🤖 will then perform the evaluation on this synthetic dataset.\n",
    ">\n",
    ">**Let’s dig into it and start building our evaluation pipeline!**\n",
    "\n",
    "### 0: Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm openai pandas langchain unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Build a synthetic dataset for evaluation\n",
    "\n",
    ">We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
    ">\n",
    ">Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Load sources\n",
    " \n",
    "We have a list of sources we use to generate questions about one source at a time using a generator function. The sources can be files or URLs.\n",
    "\n",
    "For this example our knowledge base is the NC General Statutes. We can access them by chapter or by statute through a data loading class we wrote called [KnowledgeBaseNC](knowledge_bases.py:117).\n",
    "\n",
    "More information on `KnowledgeBaseNC` and its base class [KnowledgeBase](knowledge_bases.py:16) is in Part 2. The only function we need from `KnowledgeBaseNC` in this notebook is [generate_elements()](knowledge_bases.py:131)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_bases import KnowledgeBaseNC\n",
    "\n",
    "eval_data = KnowledgeBaseNC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the sources for question generation by *chunking* them. We will do this again in Part 2 using different chunking strategies when we store embedded chunks in a vector database for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import Element\n",
    "\n",
    "\n",
    "def chunk_elements_qa(elements: list[Element]) -> list[Element]:\n",
    "    return chunk_by_title(\n",
    "        elements,\n",
    "        max_characters=10000,\n",
    "        combine_text_under_n_chars=5000,\n",
    "        new_after_n_chars=2500,\n",
    "        overlap=2000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Setup question generation agents\n",
    "\n",
    "It is necessary to call an LLM for each agent in the eval dataset generation and RAG processes, so let's import the necessary functions from our `chat_models` module. We'll use `gpt-4o` for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_models import chat, messages\n",
    "from models import ChatModelParams\n",
    "\n",
    "cm_params = ChatModelParams(engine=\"openai\", model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt that is given to our question generation agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "QA_codify_prompt = \"\"\"Your task is to come up with a question and answer about where a particular law is codified given a context.\n",
    "Your question should be answerable with a specific statute or section from the context.\n",
    "Your question should be formulated in the same style as questions users could ask a legal search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Question: (your question)\n",
    "Answer: (your answer to the question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cost and time considerations, we generate a number of questions proportional to the length of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_qa_couples(\n",
    "    chunks: list[Element],\n",
    "    max_num_questions: int,\n",
    "    min_num_chars: int,\n",
    "    chars_per_page: int,\n",
    "    max_len_answer: int,\n",
    ") -> list:\n",
    "    # filter out chunks with length < min_num_chars\n",
    "    chunks = [chunk for chunk in chunks if len(chunk.text) >= min_num_chars]\n",
    "    char_count = sum([len(chunk.text) for chunk in chunks])\n",
    "    if char_count < min_num_chars:\n",
    "        return []\n",
    "\n",
    "    question_count = min([max_num_questions, len(chunks), max([1, char_count // chars_per_page])])\n",
    "    print(f\"Generating {question_count} QA couples...\")\n",
    "\n",
    "    couples = []\n",
    "    for sampled_context in tqdm(random.sample(chunks, question_count)):\n",
    "        # Generate QA couple\n",
    "        response = chat(\n",
    "            messages(\n",
    "                [[QA_generation_prompt.format(context=sampled_context.text), None]],\n",
    "                cm_params.engine,\n",
    "            ),\n",
    "            cm_params,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        output_qa_couple = response.choices[0].message.content\n",
    "        try:\n",
    "            question = output_qa_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].rstrip()\n",
    "            answer = output_qa_couple.split(\"Answer: \")[-1].rstrip()\n",
    "            assert len(answer) < max_len_answer, \"Answer is too long\"\n",
    "            couples.append(\n",
    "                {\n",
    "                    \"context\": sampled_context.text,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": sampled_context.metadata.url,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_qa_couples(\n",
    "    n: int,\n",
    "    max_questions_per_chunk: int = 2,\n",
    "    min_chars_per_chunk: int = 2500,\n",
    "    chars_per_page: int = 2500,\n",
    "    max_len_answer: int = 1000,\n",
    "):\n",
    "    if n < max_questions_per_chunk:\n",
    "        # so we generate exactly n couples\n",
    "        max_questions_per_chunk = n\n",
    "    tot_couples = []\n",
    "    for src, elements in eval_data.generate_elements():\n",
    "        chunks = chunk_elements_qa(elements)\n",
    "        couples = generate_qa_couples(\n",
    "            chunks,\n",
    "            # so we generate exactly n couples\n",
    "            min([max_questions_per_chunk, n - len(tot_couples)]),\n",
    "            min_chars_per_chunk,\n",
    "            chars_per_page,\n",
    "            max_len_answer,\n",
    "        )\n",
    "        tot_couples += couples\n",
    "        if len(tot_couples) == n:\n",
    "            break\n",
    "    return tot_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTIONS = 30\n",
    "couples = generate_n_qa_couples(NUM_QUESTIONS)\n",
    "display(pd.DataFrame(couples).head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Setup question critique agents\n",
    "\n",
    "The generated questions can be flawed in many ways.\n",
    "\n",
    ">We use an agent to determine if a generated question meets the following criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "\n",
    "- **Groundedness**: can the question be answered from the given context?\n",
    "- **Relevance**: is the question relevant to users? For instance, *\"What are some of Thomas Jefferson's beliefs regarding the rights and liberties of individuals?\"* is not relevant for OpenProBono users.\n",
    "\n",
    ">One last failure case we’ve noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like *\"What is the name of the function used in this guide?\"*. We also build a critique agent for this criteria:\n",
    "\n",
    "- **Standalone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? For instance, *\"What does the term 'legal entity' refer to in this statute?\"* is tailored for a particular statute, but unclear by itself.\n",
    "\n",
    ">We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    ">\n",
    ">💡 ***When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.***\"\n",
    ">\n",
    ">We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to assess a question answering system.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'according to this Article', the rating must be 1.\n",
    "The questions can contain obscure legal definitions or entities like trier of fact or the North Carolina Self-Insurance Security Fund and still be a 5: it must simply be clear to an operator with access to legal documents what the question is about.\n",
    "\n",
    "For instance, the context \"On July 1 of each year, a maximum weekly benefit amount shall be computed.\" and question \"When is the maximum weekly benefit amount computed and adjusted?\" should receive a 1, since the question implicitly mentions the maximum weekly benefit amount, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the context and question.\n",
    "\n",
    "Context: {context}\\n\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_cm = ChatModelParams(engine=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "def generate_qa_critiques(couples: list[dict]):\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(couples):\n",
    "        evaluations = {\n",
    "            \"groundedness\": chat(\n",
    "                messages(\n",
    "                    [[question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), None]],\n",
    "                    critique_cm.engine,\n",
    "                ),\n",
    "                critique_cm,\n",
    "            ).choices[0].message.content,\n",
    "            \"relevance\": chat(\n",
    "                messages(\n",
    "                    [[question_relevance_critique_prompt.format(question=output[\"question\"]), None]],\n",
    "                    critique_cm.engine,\n",
    "                ),\n",
    "                critique_cm,\n",
    "            ).choices[0].message.content,\n",
    "            \"standalone\": chat(\n",
    "                messages(\n",
    "                    [[question_standalone_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), None]],\n",
    "                    critique_cm.engine,\n",
    "                ),\n",
    "                critique_cm,\n",
    "            ).choices[0].message.content,\n",
    "        }\n",
    "        try:\n",
    "            for criterion, evaluation in evaluations.items():\n",
    "                score, feedback = (\n",
    "                    int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                    evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "                )\n",
    "                output.update(\n",
    "                    {\n",
    "                        f\"{criterion}_score\": score,\n",
    "                        f\"{criterion}_eval\": feedback,\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_questions(\n",
    "    generated_questions: pd.DataFrame,\n",
    "    min_groundedness: int,\n",
    "    min_relevance: int,\n",
    "    min_standalone: int,\n",
    "):\n",
    "    return generated_questions.loc[\n",
    "        (generated_questions[\"groundedness_score\"] >= min_groundedness)\n",
    "        & (generated_questions[\"relevance_score\"] >= min_relevance)\n",
    "        & (generated_questions[\"standalone_score\"] >= min_standalone)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_qa_critiques(couples)\n",
    "couples_df = pd.DataFrame.from_dict(couples)\n",
    "display(couples_df.head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_df_filtered = filter_questions(couples_df, 4, 4, 4)\n",
    "display(couples_df_filtered.head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "Save the dataset to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"data/NC-court/court_dataset.json\").open(\"w\") as f:\n",
    "    f.write(couples_df_filtered.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
