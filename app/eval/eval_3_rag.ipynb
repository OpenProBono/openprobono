{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenProBono RAG Evaluation, Part 3\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "This notebook is for evaluating a dataset of question-answer pairs using LLM-as-a-judge to compute the accuracy of your RAG system.\n",
    "\n",
    "See Part 1 to learn to create synthetic question-answer pairs using LLMs, and Part 2 to load sources into a vector database and benchmark retrieval for Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm pandas pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pymilvus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from app.chat_models import chat\n",
    "from app.milvusdb import create_collection, query\n",
    "from app.models import ChatModelParams, EncoderParams, OpenAIModelEnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Build our RAG System\n",
    "\n",
    "We already loaded sources into our vector database in Part 2.\n",
    "\n",
    "Now we can configure an LLM and augment its generations with results from our vector database, improving its ability to answer questions about those sources.\n",
    "\n",
    "We also write functions to test the LLMs without using RAG so we can observe their baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Reader - LLM 💬\n",
    "\n",
    ">In this part, the LLM Reader reads the retrieved documents to formulate its answer.\n",
    "\n",
    "OpenProBono offers various LLM Readers using models offered by OpenAI, Anthropic, and more. We are mainly using OpenAI models right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the users question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "NORAG_PROMPT = \"\"\"You are a legal expert, tasked with answering any question about law. Give a comprehensive answer to the question.\n",
    "Respond only to the question asked. Your response should be concise and relevant to the question.\n",
    "If the answer cannot be deduced, do not give an answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question: str,\n",
    "    chat_model : ChatModelParams,\n",
    "    context: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Answer a question using an LLM.\"\"\"\n",
    "    if chat_model.engine == \"hive\":\n",
    "        msg_history = [{\"role\":\"user\", \"content\":question}]\n",
    "        answer, _ = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=0,\n",
    "            # use the rag prompt to pass any additional context from a QA evaluation set\n",
    "            system=NORAG_PROMPT if not context else RAG_PROMPT_TEMPLATE.format(\n",
    "                context=context,\n",
    "            ),\n",
    "        )\n",
    "    elif chat_model.engine == \"anthropic\":\n",
    "        msg_history = [{\"role\":\"user\", \"content\":question}]\n",
    "        response = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=0,\n",
    "            # use the rag prompt to pass any additional context from a QA evaluation set\n",
    "            system=NORAG_PROMPT if not context else RAG_PROMPT_TEMPLATE.format(\n",
    "                context=context,\n",
    "            ),\n",
    "        )\n",
    "        answer = response.content[-1].text\n",
    "    else:\n",
    "        msg_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": NORAG_PROMPT if not context else RAG_PROMPT_TEMPLATE.format(\n",
    "                    context=context,\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "        response = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=0,\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    chat_model: ChatModelParams,\n",
    "    collection_name: str | None = None,\n",
    "    k: int = 4,\n",
    ") -> tuple[str, list[dict]]:\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = query(collection_name, question, k)\n",
    "    relevant_docs = [doc[\"entity\"][\"text\"] for doc in relevant_docs[\"result\"]]  # keep only the text\n",
    "\n",
    "    # Build the final prompt context\n",
    "    context = \"\".join([f\"Document {i!s}:::\\n\" + doc.rstrip() for i, doc in enumerate(relevant_docs)])\n",
    "    answer = answer_question(question, chat_model, context)\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Benchmarking the RAG System\n",
    "\n",
    ">The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    ">\n",
    ">To this end, **we setup a judge agent**. ⚖️🤖\n",
    ">\n",
    ">Out of the different RAG evaluation metrics, we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    ">💡 *In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in Prometheus's prompt template: this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples.*\n",
    ">\n",
    ">💡 *Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa(\n",
    "    chat_model: ChatModelParams,\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    output_file: str,\n",
    "    collection_name: str | None = None,\n",
    "    verbose: bool | None = True,\n",
    "    test_settings: str | None = None,  # To document the test settings used\n",
    "    use_embedding: bool | None = True,\n",
    "    synth_data: bool | None = True,\n",
    "):\n",
    "    \"\"\"Run a chat model through the given Q&A dataset and save the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with Path(output_file).open() as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for _, example in tqdm(eval_dataset.iterrows(), total=len(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        if use_embedding:\n",
    "            answer, relevant_docs = answer_with_rag(question, chat_model, collection_name)\n",
    "        else:\n",
    "            # change/remove context if necessary\n",
    "            answer = answer_question(question, chat_model, example[\"contract\"])\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"generated_answer\": answer,\n",
    "        }\n",
    "        if synth_data:\n",
    "            result[\"source_doc\"] = example[\"source_doc\"]\n",
    "        if use_embedding and relevant_docs:\n",
    "            result[\"retrieved_docs\"] = list(relevant_docs)\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with Path(output_file).open(\"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MSG = \"You are a fair evaluator language model.\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_temperature = 0\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    evaluator_llm: ChatModelParams,\n",
    ") -> None:\n",
    "    \"\"\"Evaluate generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if Path(answer_path).is_file():  # load previous generations if they exist\n",
    "        with Path(answer_path).open() as f:\n",
    "            answers = json.load(f)\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_llm.model}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = EVALUATION_PROMPT.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_sys_msg = {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MSG}\n",
    "        eval_msg = {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        eval_response = chat([eval_sys_msg, eval_msg], evaluator_llm, temperature=eval_temperature)\n",
    "        eval_result = eval_response.choices[0].message.content\n",
    "        feedback, score = (item.strip() for item in eval_result.split(\"[RESULT]\"))\n",
    "        experiment[f\"eval_score_{evaluator_llm.model}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_llm.model}\"] = feedback\n",
    "\n",
    "        with Path(answer_path).open(mode=\"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">🚀 Let's run the tests and evaluate answers!👇\n",
    "\n",
    "Read the dataset from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.eval.evaluations import legalbench\n",
    "\n",
    "couples_df = legalbench(\"consumer_contracts_qa\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import UTC, datetime\n",
    "\n",
    "from app.knowledge_bases import KnowledgeBaseNC\n",
    "\n",
    "eval_data = KnowledgeBaseNC()\n",
    "gpt_3_5 = ChatModelParams(engine=\"openai\", model=OpenAIModelEnum.gpt_3_5)\n",
    "gpt_4o = ChatModelParams(engine=\"openai\", model=OpenAIModelEnum.gpt_4_o)\n",
    "hive_7b = ChatModelParams(engine=\"hive\", model=\"hive-7b\")\n",
    "hive_70b = ChatModelParams(engine=\"hive\", model=\"hive-70b\")\n",
    "claude = ChatModelParams(engine=\"anthropic\", model=\"claude-3-sonnet-20240229\")\n",
    "evaluator_llm = gpt_4o\n",
    "reader_models = [gpt_3_5, gpt_4o, hive_7b, hive_70b, claude]\n",
    "use_embedding = False # change to True if you want to enable RAG\n",
    "filenames = {\n",
    "    gpt_3_5.model: \"gpt_35\",\n",
    "    gpt_4o.model: \"gpt_4o\",\n",
    "    hive_7b.model: \"hive-7b\",\n",
    "    hive_70b.model: \"hive-70b\",\n",
    "    claude.model: \"claude-sonnet\",\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    chat_model: ChatModelParams,\n",
    "    settings_name: str,\n",
    "    collection_name: str | None = None,\n",
    "    use_embedding: bool | None = None, # To answer questions with RAG or not\n",
    "    synth_data: bool | None = None, # To include source_doc in eval result if available\n",
    "):\n",
    "    # run evaluations with a configured knowledge base\n",
    "    output_file_name = f\"data/consumer_contractqa/{settings_name}.json\"\n",
    "\n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "    print(\"Running RAG...\")\n",
    "    run_qa(\n",
    "        chat_model=chat_model,\n",
    "        eval_dataset=couples_df,\n",
    "        output_file=output_file_name,\n",
    "        collection_name=collection_name,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "        use_embedding=use_embedding,\n",
    "        synth_data=synth_data,\n",
    "    )\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "    evaluate_answers(\n",
    "        output_file_name,\n",
    "        evaluator_llm,\n",
    "    )\n",
    "\n",
    "\n",
    "if use_embedding:\n",
    "    vdb_basename = \"Eval_\" + datetime.now(UTC).strftime(\"%Y%m%d\")\n",
    "    idx = 1\n",
    "    for encoder in [EncoderParams(name=OpenAIModelEnum.embed_small, dim=768)]:\n",
    "        for chunk_hardmax, chunk_softmax, overlap in [(5000, 2000, 500)]:\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            collection_name = f\"{vdb_basename}_{idx}\"\n",
    "            idx += 1\n",
    "            if not pymilvus.utility.has_collection(collection_name):\n",
    "                description = (\n",
    "                    f\"Hardmax = {chunk_hardmax}, \"\n",
    "                    f\"Softmax = {chunk_softmax}, \"\n",
    "                    f\"Overlap = {overlap}, \"\n",
    "                    f\"Encoder = {encoder.name}.\"\n",
    "                )\n",
    "                create_collection(\n",
    "                    collection_name,\n",
    "                    encoder,\n",
    "                    description,\n",
    "                )\n",
    "                eval_data.populate_database(\n",
    "                    collection_name,\n",
    "                    chunk_hardmax,\n",
    "                    chunk_softmax,\n",
    "                    overlap,\n",
    "                )\n",
    "            for llm in reader_models:\n",
    "                settings_name = filenames[llm.model]\n",
    "                evaluate_model(\n",
    "                    llm,\n",
    "                    settings_name,\n",
    "                    collection_name,\n",
    "                )\n",
    "else:\n",
    "    for llm in reader_models:\n",
    "        settings_name = filenames[llm.model]\n",
    "        evaluate_model(llm, settings_name, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average evaluation scores as accuracy percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_gpt35_employ_norag = pd.read_json(\"data/consumer_contractqa/gpt_35.json\", orient='records')[\"eval_score_gpt-4o\"]\n",
    "results_gpt4o_employ_norag = pd.read_json(\"data/consumer_contractqa/gpt_4o.json\", orient='records')[\"eval_score_gpt-4o\"]\n",
    "results_hive7b_employ_norag = pd.read_json(\"data/consumer_contractqa/hive-7b.json\", orient='records')[\"eval_score_gpt-4o\"]\n",
    "results_hive70b_employ_norag = pd.read_json(\"data/consumer_contractqa/hive-70b.json\", orient='records')[\"eval_score_gpt-4o\"]\n",
    "results_claude_employ_norag = pd.read_json(\"data/consumer_contractqa/claude-sonnet.json\", orient='records')[\"eval_score_gpt-4o\"]\n",
    "\n",
    "\n",
    "# Define the x-axis labels and values (evaluation models)\n",
    "models = [\"gpt-3.5-turbo-0125\", \"gpt-4o\", \"hive-7b\", \"hive-70b\", \"claude-3-sonnet\"]\n",
    "# Define the y-axis values (average scores)\n",
    "y_values = [\n",
    "    (results_gpt35_employ_norag.mean() / 5) * 100,\n",
    "    (results_gpt4o_employ_norag.mean() / 5) * 100,\n",
    "    (results_hive7b_employ_norag.mean() / 5) * 100,\n",
    "    (results_hive70b_employ_norag.mean() / 5) * 100,\n",
    "    (results_claude_employ_norag.mean() / 5) * 100,\n",
    "]\n",
    "# Define some colors to make it pretty\n",
    "colors = [\"lightgreen\", \"green\", \"lightblue\", \"blue\", \"brown\"]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(models, y_values, color=colors)\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Evaluation Accuracy Comparison - GPT-4o Judge\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average Evaluation Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
