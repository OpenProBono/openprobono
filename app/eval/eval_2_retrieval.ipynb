{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenProBono RAG Evaluation, Part 2\n",
    "## Retrieval\n",
    "\n",
    "This notebook is for turning a knowledge base into a vector database, and evaluating retrieval over the database using question-answer pairs.\n",
    "\n",
    "If you don't have any question-answer pairs, see Part 1 to create synthetic question-answer pairs from your knowledge base using LLMs.\n",
    "\n",
    "### 0: Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm pandas pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pymilvus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from app.models import ChatModelParams, EncoderParams, OpenAIModelEnum, VoyageModelEnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Build our Vector Database\n",
    "\n",
    "We load data from our knowledge base into a Collection in our Milvus vector database.\n",
    "\n",
    "To accomplish this, we must:\n",
    "\n",
    "1. create a vector database\n",
    "2. extract content from our knowledge base\n",
    "2. chunk the extracted content\n",
    "3. embed the chunks\n",
    "4. insert embeddings into the database\n",
    "\n",
    "Step 1 is fairly straightforward, and steps 2-5 are done inside of a data loading class called [KnowledgeBase](../knowledge_bases.py#L17).\n",
    "\n",
    "To use the data loading class, we need to make a subclass of it and implement the [generate_elements()](../knowledge_bases.py#L21) function for step 2. This function extracts content from our sources and returns a tuple with a source name and its extracted content.\n",
    "\n",
    "The entire process is executed in the [populate_database()](../knowledge_bases.py#L37) function.\n",
    "\n",
    "We have an example implementation using the NC General Statutes called [KnowledgeBaseNC](../knowledge_bases.py#L122). Let's break down each step and discuss the relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.knowledge_bases import KnowledgeBaseNC\n",
    "\n",
    "eval_data = KnowledgeBaseNC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0: Create vector database\n",
    "\n",
    "First things first, we should create an empty database to hold our content after we prepare it.\n",
    "\n",
    "The content is converted into document embeddings and stored in a vector database for fast vector searching during RAG.\n",
    "\n",
    "Below is a function that creates a vector database we can use to store documents.\n",
    "\n",
    "The only required parameter is the `name` of the database, but we should also specify an embedding model using the [EncoderParams](../models.py#L220) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.milvusdb import create_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Chunk sources into documents\n",
    "\n",
    ">- In this part, **we split the documents from our knowledge base into smaller chunks**: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    ">- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    ">\n",
    ">Many options exist for text splitting:\n",
    ">\n",
    ">- split every n words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    ">- split after n words / character, but only on sentence boundaries\n",
    ">- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    ">\n",
    ">To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    ">\n",
    ">[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "As mentioned previously, the [populate_database()](../knowledge_bases.py#L37) function contains the code for extracting, chunking, embedding, and uploading documents.\n",
    "\n",
    "The objects returned from `generate_elements()` are passed into the chunking function, [chunk_by_title()](../knowledge_bases.py#L67)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Embed documents\n",
    "\n",
    ">The retriever acts like an internal search engine: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "An embedding model transforms documents into vectors, and Milvus creates an index over the vectors for fast and accurate retrieval.\n",
    "\n",
    "We transform documents via the [embed_strs()](../encoders.py#L80) function that accepts a list of strings and an `EncoderParams` object and returns a list of vectors.\n",
    "\n",
    "This function gets called from [populate_database()](../knowledge_bases.py#L78) after chunking and before uploading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Insert documents\n",
    "\n",
    "Now that we've chunked and embedded our documents, the last step is to insert the document embeddings and metadata into the newly created vector database.\n",
    "\n",
    "An embedded document is represented by its corresponding `vector`, `text`, and `metadata`.\n",
    "\n",
    "The [upload_data()](../milvusdb.py#L356) accepts a list of dictionaries containing this data and a destination `collection_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Populate the vector database\n",
    "\n",
    "So far, the parameters we can control are:\n",
    "\n",
    "1. Chunking strategy parameters\n",
    "    - chunk hardmax: the maximum number of characters in a chunk\n",
    "    - chunk softmax: the preferred maximum number of characters in a chunk (see `new_after_n_chars` in `chunk_by_title` for more)\n",
    "    - overlap: the number of characters to overlap between consecutive chunks\n",
    "2. Embedding model\n",
    "3. The number of documents to retrieve for a query (k)\n",
    "\n",
    "There are also some parameters we can't currently control:\n",
    "\n",
    "1. Document loader (unstructured)\n",
    "2. Base chunking strategy (`chunk_by_title`)\n",
    "3. Similarity metric (normalized inner product ~ cosine)\n",
    "3. Vector Index (Zilliz autoindex)\n",
    "4. Relevancy threshold (currently not set, but could be anywhere between 0 and 2 assuming embeddings are normalized and cosine distance is used)\n",
    "5. Reranking (distance is the default ranking)\n",
    "\n",
    "Once we decide on values for the parameters we can control, we can chunk and embed our sources into a vector database. We can test the retrieval component of the system separately from generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5: Query the vector database\n",
    "\n",
    "The last step to benchmark is to call a function to retrieve vectors based on semantic similarity to a query. This function accepts a `collection_name`, a `query` string, and the number `k` of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.milvusdb import query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6: Benchmark retrieval\n",
    "\n",
    "There are many ways to test retrieval from a vector database. We will use two simple metrics for now:\n",
    "\n",
    "1. Mean Reciprocal Rank (MRR) - where (what rank) does the first relevant document show up in our list of retrieved documents?\n",
    "2. Precision @ K - out of the K documents we retrieve, how many are relevant?\n",
    "\n",
    "These metrics rely on classifying a document as relevant or irrelevant, and we can use LLMs for this.\n",
    "\n",
    "Let's write a prompt to classify a bit of context as relevant or irrelevant towards answering a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_RELEVANCE_PROMPT = \"\"\"You are comparing a reference text to a question and trying to determine if the reference text contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Reference text]: {context}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the Reference text. You must determine whether the Reference text contains information that can answer the Question.\n",
    "Please focus on whether the very specific question can be answered by the information in the Reference text.\n",
    "Your response must be single word, either \"relevant\" or \"unrelated\", and should not contain any text or characters aside from that word.\n",
    "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "\"relevant\" means the reference text contains an answer to the Question.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write the function to call the LLM to perform this classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_models import chat\n",
    "\n",
    "eval_temperature = 0\n",
    "\n",
    "def classify_context(chat_model: ChatModelParams, question: str, context: str) -> str:\n",
    "    if chat_model.engine == \"hive\":\n",
    "        msg_history = [{\"role\":\"user\", \"content\":question}]\n",
    "        answer, _ = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=eval_temperature,\n",
    "            system=CONTEXT_RELEVANCE_PROMPT.format(\n",
    "                question=question,\n",
    "                context=context,\n",
    "            ),\n",
    "        )\n",
    "    elif chat_model.engine == \"anthropic\":\n",
    "        msg_history = [{\"role\":\"user\", \"content\":question}]\n",
    "        response = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=eval_temperature,\n",
    "            system=CONTEXT_RELEVANCE_PROMPT.format(\n",
    "                question=question,\n",
    "                context=context,\n",
    "            ),\n",
    "        )\n",
    "        answer = response.content[-1].text\n",
    "    else:\n",
    "        msg_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": CONTEXT_RELEVANCE_PROMPT.format(\n",
    "                    question=question,\n",
    "                    context=context,\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "        response = chat(\n",
    "            msg_history,\n",
    "            chat_model,\n",
    "            temperature=eval_temperature,\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a function to perform the queries on our vector database using the questions in the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(\n",
    "    collection_name: str,\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    k: int,\n",
    "    output_file: str,\n",
    "    synth_data: bool = True,\n",
    "):\n",
    "    try:  # load previous generations if they exist\n",
    "        with Path(output_file).open() as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for _, example in tqdm(eval_dataset.iterrows(), total=len(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = query(collection_name, question, k)\n",
    "        if not relevant_docs[\"result\"]:\n",
    "            print(\"ERROR: no results found\")\n",
    "            continue\n",
    "        # keep only text and distance\n",
    "        relevant_docs = [\n",
    "            {\n",
    "                \"text\": doc[\"entity\"][\"text\"],\n",
    "                \"distance\": doc[\"distance\"],\n",
    "            }\n",
    "            for doc in relevant_docs[\"result\"]\n",
    "        ]\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"retrieved_docs\": relevant_docs,\n",
    "        }\n",
    "        if synth_data:\n",
    "            result[\"source\"] = example[\"context\"]\n",
    "        outputs.append(result)\n",
    "\n",
    "        with Path(output_file).open(\"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run the retrieval, we perform our evaluation by classifying each retrieved document for each question.\n",
    "\n",
    "If synthetic data was provided that has the source text the question/answer pair was generated from, we can further evaluate our retrieved documents.\n",
    "\n",
    "We can compute the longest common substring between the source text and each document and divide this number by the length of the document.\n",
    "\n",
    "This measures how much of the retrieved document is in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_substring(s1: str, s2: str):\n",
    "    m = [[0] * (1 + len(s2)) for _ in range(1 + len(s1))]\n",
    "    longest = 0\n",
    "    for x in range(1, 1 + len(s1)):\n",
    "        for y in range(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "    return longest\n",
    "\n",
    "\n",
    "def evaluate_retrieval(chat_model: ChatModelParams, retrieval_path: str, synth_data: bool = True):\n",
    "    \"\"\"Evaluate retrieval. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    retrievals = []\n",
    "    if Path(retrieval_path).is_file():  # load retrieval\n",
    "        with Path(retrieval_path).open() as f:\n",
    "            retrievals = json.load(f)\n",
    "\n",
    "    for experiment in tqdm(retrievals):\n",
    "        contexts = experiment[\"retrieved_docs\"]\n",
    "        first_relevant, num_relevant = 0, 0\n",
    "        for i, context in enumerate(contexts):\n",
    "            if f\"eval_{chat_model.model}\" not in context:\n",
    "                # get the relevant/irrelevant classification from the LLM\n",
    "                answer = classify_context(chat_model, experiment[\"question\"], context[\"text\"])\n",
    "                context[f\"eval_{chat_model.model}\"] = answer\n",
    "            if \"eval_gpt-4o\" in context and context[\"eval_gpt-4o\"] == \"relevant\":\n",
    "                num_relevant += 1\n",
    "                if first_relevant == 0:\n",
    "                    first_relevant = i + 1\n",
    "\n",
    "            if synth_data and \"source_lcspct\" not in context:\n",
    "                # get longest common substring / length of retrieved context\n",
    "                lcs = longest_common_substring(experiment[\"source\"], context[\"text\"])\n",
    "                context[\"source_lcspct\"] = lcs / len(context[\"text\"])\n",
    "\n",
    "            with Path(retrieval_path).open(mode=\"w\") as f:\n",
    "                json.dump(retrievals, f)\n",
    "\n",
    "        # overall evaluation metrics MRR and Precision @ k\n",
    "        experiment[\"rr\"] = 1 / first_relevant if first_relevant > 0 else 0\n",
    "        experiment[\"precision\"] = num_relevant / len(contexts)\n",
    "\n",
    "    # write the last row results to file\n",
    "    with Path(retrieval_path).open(mode=\"w\") as f:\n",
    "        json.dump(retrievals, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your `DataFrame` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/NC-employment\"\n",
    "couples_df = pd.read_json(f\"{data_dir}/employment_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, run the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = ChatModelParams(engine=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "def evaluate_embedding(\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    collection_name: str,\n",
    "    chunk_hardmax: int,\n",
    "    chunk_softmax: int,\n",
    "    overlap: int,\n",
    "    k: int,\n",
    "    encoder: EncoderParams,\n",
    "    synth_data: bool = True,\n",
    "):\n",
    "    # run evaluations with a configured knowledge base\n",
    "    settings_name = (\n",
    "        f\"collection_name:{collection_name}\"\n",
    "        f\"hardmax:{chunk_hardmax}\"\n",
    "        f\"softmax:{chunk_softmax}\"\n",
    "        f\"_overlap:{overlap}\"\n",
    "        f\"_k:{k}_encoder:{encoder.name}-{encoder.dim}\"\n",
    "    )\n",
    "    output_file_name = f\"{data_dir}/{settings_name}.json\"\n",
    "\n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "    print(\"Running retrieval...\")\n",
    "    run_retrieval(\n",
    "        collection_name,\n",
    "        eval_dataset,\n",
    "        k,\n",
    "        output_file_name,\n",
    "        synth_data,\n",
    "    )\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "    evaluate_retrieval(evaluator_llm, output_file_name, synth_data)\n",
    "\n",
    "vdb_basename = \"Eval_\" + datetime.now(UTC).strftime(\"%Y%m%d\")\n",
    "idx = 1\n",
    "for encoder in [EncoderParams(name=VoyageModelEnum.law, dim=1024)]:\n",
    "    for chunk_hardmax, chunk_softmax, overlap in [(5000, 2000, 500)]:\n",
    "        print(\"Loading knowledge base embeddings...\")\n",
    "        collection_name = \"Voyage_Courtroom5_NCStatutesPDF\" # f\"{vdb_basename}_{idx}\"\n",
    "        idx += 1\n",
    "        if not pymilvus.utility.has_collection(collection_name):\n",
    "            description = (\n",
    "                f\"Hardmax = {chunk_hardmax}, \"\n",
    "                f\"Softmax = {chunk_softmax}, \"\n",
    "                f\"Overlap = {overlap}, \"\n",
    "                f\"Encoder = {encoder.name}.\"\n",
    "            )\n",
    "            create_collection(\n",
    "                collection_name,\n",
    "                encoder,\n",
    "                description,\n",
    "            )\n",
    "            eval_data.populate_database(\n",
    "                collection_name,\n",
    "                chunk_hardmax,\n",
    "                chunk_softmax,\n",
    "                overlap,\n",
    "            )\n",
    "        for k in [2, 4, 6, 8]:\n",
    "            evaluate_embedding(\n",
    "                couples_df,\n",
    "                collection_name,\n",
    "                chunk_hardmax,\n",
    "                chunk_softmax,\n",
    "                overlap,\n",
    "                k,\n",
    "                encoder,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the results in from output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_k2 = pd.read_json(data_dir + \"/collection_name:Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:2_encoder:text-embedding-3-small-768.json\", orient='records')\n",
    "results_k4 = pd.read_json(data_dir + \"/collection_name:Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:4_encoder:text-embedding-3-small-768.json\", orient='records')\n",
    "results_k6 = pd.read_json(data_dir + \"/collection_name:Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:6_encoder:text-embedding-3-small-768.json\", orient='records')\n",
    "results_k8 = pd.read_json(data_dir + \"/collection_name:Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:8_encoder:text-embedding-3-small-768.json\", orient='records')\n",
    "results_voyage_k2 = pd.read_json(data_dir + \"/collection_name:Voyage_Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:2_encoder:voyage-law-2-1024.json\", orient='records')\n",
    "results_voyage_k4 = pd.read_json(data_dir + \"/collection_name:Voyage_Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:4_encoder:voyage-law-2-1024.json\", orient='records')\n",
    "results_voyage_k6 = pd.read_json(data_dir + \"/collection_name:Voyage_Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:6_encoder:voyage-law-2-1024.json\", orient='records')\n",
    "results_voyage_k8 = pd.read_json(data_dir + \"/collection_name:Voyage_Courtroom5_NCStatutesPDFhardmax:5000softmax:2000_overlap:500_k:8_encoder:voyage-law-2-1024.json\", orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the x-axis labels and values (K)\n",
    "#ks = [\"2\", \"4\", \"6\", \"8\"]\n",
    "encoders = [\"o2\", \"v2\", \"o4\", \"v4\", \"o6\", \"v6\", \"o8\", \"v8\"]\n",
    "# Define the y-axis values (average scores)\n",
    "y_values = [\n",
    "    results_k2[\"rr\"].mean(),\n",
    "    results_voyage_k2[\"rr\"].mean(),\n",
    "    results_k4[\"rr\"].mean(),\n",
    "    results_voyage_k4[\"rr\"].mean(),\n",
    "    results_k6[\"rr\"].mean(),\n",
    "    results_voyage_k6[\"rr\"].mean(),\n",
    "    results_k8[\"rr\"].mean(),\n",
    "    results_voyage_k8[\"rr\"].mean(),\n",
    "]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(encoders, y_values)\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Retrieval Evaluation - Mean Reciprocal Rank (MRR)\")\n",
    "plt.xlabel(\"Embedding Model\")\n",
    "plt.ylabel(\"MRR\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Precision @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the y-axis values (average scores)\n",
    "y_values = [\n",
    "    results_k2[\"precision\"].mean(),\n",
    "    results_voyage_k2[\"precision\"].mean(),\n",
    "    results_k4[\"precision\"].mean(),\n",
    "    results_voyage_k4[\"precision\"].mean(),\n",
    "    results_k6[\"precision\"].mean(),\n",
    "    results_voyage_k6[\"precision\"].mean(),\n",
    "    results_k8[\"precision\"].mean(),\n",
    "    results_voyage_k8[\"precision\"].mean(),\n",
    "]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(encoders, y_values)\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Retrieval Evaluation - Mean Precision\")\n",
    "plt.xlabel(\"Embedding Model\")\n",
    "plt.ylabel(\"Mean Precision\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the average percentage of a chunk's text that is a substring of the source text that generated the question/answer pair (only available if synthetic or Q/A source data was provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs_k2 = [sublist[i][\"source_lcspct\"] for sublist in results_k2[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_k4 = [sublist[i][\"source_lcspct\"] for sublist in results_k4[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_k6 = [sublist[i][\"source_lcspct\"] for sublist in results_k6[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_k8 = [sublist[i][\"source_lcspct\"] for sublist in results_k8[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_voyage_k2 = [sublist[i][\"source_lcspct\"] for sublist in results_voyage_k2[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_voyage_k4 = [sublist[i][\"source_lcspct\"] for sublist in results_voyage_k4[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_voyage_k6 = [sublist[i][\"source_lcspct\"] for sublist in results_voyage_k6[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "lcs_voyage_k8 = [sublist[i][\"source_lcspct\"] for sublist in results_voyage_k8[\"retrieved_docs\"] for i in range(len(sublist))]\n",
    "\n",
    "# Define the y-axis values\n",
    "y_values = [\n",
    "    (sum(lcs_k2) / len(lcs_k2)) * 100,\n",
    "    (sum(lcs_voyage_k2) / len(lcs_voyage_k2)) * 100,\n",
    "    (sum(lcs_k4) / len(lcs_k4)) * 100,\n",
    "    (sum(lcs_voyage_k4) / len(lcs_voyage_k4)) * 100,\n",
    "    (sum(lcs_k6) / len(lcs_k6)) * 100,\n",
    "    (sum(lcs_voyage_k6) / len(lcs_voyage_k6)) * 100,\n",
    "    (sum(lcs_k8) / len(lcs_k8)) * 100,\n",
    "    (sum(lcs_voyage_k8) / len(lcs_voyage_k8)) * 100,\n",
    "]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(encoders, y_values)\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Retrieval Evaluation - Longest Common Substring %\")\n",
    "plt.xlabel(\"Embedding Model\")\n",
    "plt.ylabel(\"Average % Substring\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
