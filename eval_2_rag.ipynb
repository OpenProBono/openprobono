{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenProBono RAG Evaluation, Part 2\n",
    "\n",
    "This notebook is for evaluating a dataset of question-answer pairs using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "See Part 1 to learn to create synthetic question-answer pairs using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm openai pandas langchain unstructured pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pymilvus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import ChatModelParams, EncoderParams, OpenAIModelEnum\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Build our RAG System\n",
    "\n",
    "We load the data for RAG into a `Collection` in our Milvus vector database.\n",
    "\n",
    "To accomplish this, we must:\n",
    "\n",
    "1. extract the content from our sources\n",
    "2. chunk the extracted content\n",
    "3. embed the chunks\n",
    "4. insert them to the database\n",
    "\n",
    "We already wrote generator functions to extract the content, `generate_statute_elements` and `generate_chapter_elements`, so our next step is chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Chunk sources into documents\n",
    "\n",
    ">- In this part, **we split the documents from our knowledge base into smaller chunks**: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    ">- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    ">\n",
    ">Many options exist for text splitting:\n",
    ">\n",
    ">- split every n words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    ">- split after n words / character, but only on sentence boundaries\n",
    ">- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    ">\n",
    ">To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    ">\n",
    ">[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "We need a function to chunk the content in a given source into documents. This function should accept a list of items containing extracted text and relevant metadata from the source. In our generator function, `partition` returns a list of `Element` objects. We pass these into a chunking function, `chunk_by_title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Embed documents\n",
    "\n",
    ">The retriever acts like an internal search engine: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "An embedding model transforms the documents to vectors, and Milvus creates an index over those vectors for fast and accurate retrieval.\n",
    "\n",
    "We want to evaluate different embedding models, so we represent them with a function that accepts text and returns vectors. In order to easily test different models, the function also accepts an `EncoderParams` object that tells it which embedding model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import embed_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3: Create vector database\n",
    "\n",
    "The document embeddings are stored in a vector database for fast vector searching during RAG.\n",
    "\n",
    "Below is a function that creates a vector database we can use to store documents, and a function to retrieve the embedded documents. The `name` of the database will be determined by the chosen sources, chunking strategy, and embedding model evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milvusdb import create_collection, query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Insert documents\n",
    "\n",
    "The last step is to insert the embedded documents into the newly created vector database.\n",
    "\n",
    "An embedded document is represented by its corresponding `vectors`, `texts`, and `metadatas`. The vector database is given by a Milvus `Collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_BATCH_SIZE = 100\n",
    "\n",
    "def insert_documents(vectors: list, texts: list, metadatas: list, collection: pymilvus.Collection):\n",
    "    data = [vectors, texts, metadatas]\n",
    "    num_chunks = len(vectors)\n",
    "    pks = []\n",
    "    for i in range(0, num_chunks, INSERT_BATCH_SIZE):\n",
    "        batch_vector = data[0][i: i + INSERT_BATCH_SIZE]\n",
    "        batch_text = data[1][i: i + INSERT_BATCH_SIZE]\n",
    "        batch_metadata = data[2][i: i + INSERT_BATCH_SIZE]\n",
    "        batch = [batch_vector, batch_text, batch_metadata]\n",
    "        current_batch_size = len(batch[0])\n",
    "        res = collection.insert(batch)\n",
    "        pks += res.primary_keys\n",
    "        if res.insert_count != current_batch_size:\n",
    "            # the upload failed, try deleting any partially uploaded data\n",
    "            bad_deletes = []\n",
    "            for pk in pks:\n",
    "                delete_res = collection.delete(expr=f\"pk=={pk}\")\n",
    "                if delete_res.delete_count != 1:\n",
    "                    bad_deletes.append(pk)\n",
    "            bad_insert = (\n",
    "                f\"Failure: expected {current_batch_size} insertions but got \"\n",
    "                f\"{res.insert_count}. \"\n",
    "            )\n",
    "            if bad_deletes:\n",
    "                bad_insert += f\"Dangling data: {','.join(bad_deletes)}\"\n",
    "            else:\n",
    "                bad_insert += \"Any partially uploaded data has been deleted.\"\n",
    "            return {\"message\": bad_insert}\n",
    "    return {\"message\": \"Success\", \"num_chunks\": num_chunks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Populate the vector database\n",
    "\n",
    "So far, the parameters we can control are:\n",
    "\n",
    "1. Chunking strategy parameters\n",
    "    - chunk hardmax: the maximum number of characters in a chunk\n",
    "    - chunk softmax: the preferred maximum number of characters in a chunk (see `new_after_n_chars` in `chunk_by_title` for more)\n",
    "    - overlap: the number of characters to overlap between consecutive chunks\n",
    "2. Embedding model\n",
    "\n",
    "There are also some parameters we can't currently control:\n",
    "\n",
    "1. Document loader (unstructured)\n",
    "2. Question generation chunking strategy (LangChain's `RecursiveCharacterTextSplitter`)\n",
    "2. Base RAG chunking strategy (unstructured's `chunk_by_title`)\n",
    "3. Vector Index (Zilliz autoindex)\n",
    "\n",
    "Once we decide on values for the parameters we can control, we can chunk and embed our sources into a vector database. We can then use the database to generate a synthetic dataset of question-answer pairs and/or evaluate agents' responses to a generated (or loaded) dataset of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluations import generate_chapter_elements\n",
    "\n",
    "\n",
    "def populate_database(collection: pymilvus.Collection, chunk_hardmax: int, chunk_softmax: int, overlap: int, encoder: EncoderParams):\n",
    "    for src, elements in generate_chapter_elements():\n",
    "        chunks = chunk_by_title(\n",
    "            elements,\n",
    "            max_characters=chunk_hardmax,\n",
    "            new_after_n_chars=chunk_softmax,\n",
    "            overlap=overlap,\n",
    "        )\n",
    "        num_chunks = len(chunks)\n",
    "        texts, metadatas = [], []\n",
    "        for i in range(num_chunks):\n",
    "            texts.append(chunks[i].text)\n",
    "            metadatas.append(chunks[i].metadata.to_dict())\n",
    "        vectors = embed_strs(texts, encoder)\n",
    "        result = insert_documents(vectors, texts, metadatas, collection)\n",
    "        if result[\"message\"] != \"Success\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to rerun this notebook with data that has already been inserted to Milvus. If so, we need different generator functions to load the data from Milvus rather than the source URLs themselves. We can call them through `vdb_source_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluations import vdb_source_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Reader - LLM 💬\n",
    "\n",
    ">In this part, the LLM Reader reads the retrieved documents to formulate its answer.\n",
    "\n",
    "OpenProBono offers various LLM Readers using models offered by OpenAI, HuggingFace, and more. We will create a simple LLM Reader connected to the OpenAI API for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT_UNFORMATTED = (\n",
    "    \"Using the information contained in the context, \"\n",
    "    \"give a comprehensive answer to the question. \"\n",
    "    \"Respond only to the question asked, response should be concise and relevant to the question. \"\n",
    "    \"Provide the number of the source document when relevant. \"\n",
    "    \"If the answer cannot be deduced from the context, do not give an answer.\"\n",
    ")\n",
    "\n",
    "VANILLA_PROMPT = (\n",
    "    \"Give a comprehensive answer to the question. \"\n",
    "    \"Respond only to the question asked, response should be concise and relevant to the question. \"\n",
    "    \"If the answer cannot be deduced, do not give an answer.\"\n",
    "    \"\\n\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_models import chat, messages\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    collection_name: str,\n",
    "    question: str,\n",
    "    chat_model: ChatModelParams,\n",
    "    k: int = 7,\n",
    ") -> tuple[str, list[dict]]:\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = query(collection_name, question, k)\n",
    "    relevant_docs = [doc[\"entity\"][\"text\"] for doc in relevant_docs[\"result\"]]  # keep only the text\n",
    "\n",
    "    # Build the final prompt context\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {i!s}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    # Redact an answer\n",
    "    response = chat(\n",
    "        messages(\n",
    "            [[RAG_PROMPT_TEMPLATE.format(question=question, context=context), None]],\n",
    "            chat_model.engine,\n",
    "        ),\n",
    "        chat_model,\n",
    "        temperature=0,\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Benchmarking the RAG System\n",
    "\n",
    ">The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    ">\n",
    ">To this end, **we setup a judge agent**. ⚖️🤖\n",
    ">\n",
    ">Out of the different RAG evaluation metrics, we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    ">💡 *In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in Prometheus's prompt template: this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples.*\n",
    ">\n",
    ">💡 *Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_tests(\n",
    "    chat_model: ChatModelParams,\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    collection_name: str,\n",
    "    output_file: str,\n",
    "    verbose: bool | None = True,\n",
    "    test_settings: str = \"\",  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Run RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with Path(output_file).open() as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for _, example in tqdm(eval_dataset.iterrows(), total=len(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(collection_name, question, chat_model)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": list(relevant_docs),\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with Path(output_file).open(\"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MSG = \"You are a fair evaluator language model.\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_temperature = 0\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    evaluator_llm: ChatModelParams,\n",
    ") -> None:\n",
    "    \"\"\"Evaluate generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if Path(answer_path).is_file():  # load previous generations if they exist\n",
    "        with Path(answer_path).open() as f:\n",
    "            answers = json.load(f)\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_llm.model}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = EVALUATION_PROMPT.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_sys_msg = {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MSG}\n",
    "        eval_msg = {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        eval_response = chat([eval_sys_msg, eval_msg], evaluator_llm, temperature=eval_temperature)\n",
    "        eval_result = eval_response.choices[0].message.content\n",
    "        feedback, score = (item.strip() for item in eval_result.split(\"[RESULT]\"))\n",
    "        experiment[f\"eval_score_{evaluator_llm.model}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_llm.model}\"] = feedback\n",
    "\n",
    "        with Path(answer_path).open(mode=\"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">🚀 Let's run the tests and evaluate answers!👇\n",
    "\n",
    "Read the dataset from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_df = pd.read_json(\"data/NC-court/court_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import UTC, datetime\n",
    "\n",
    "vdb_basename = \"CourtEval_\" + datetime.now(UTC).strftime(\"%Y%m%d\")\n",
    "idx = 1\n",
    "gpt_3_5 = ChatModelParams(engine=\"openai\", model=OpenAIModelEnum.gpt_3_5)\n",
    "gpt_4o = ChatModelParams(engine=\"openai\", model=OpenAIModelEnum.gpt_4_o)\n",
    "hive_7b = ChatModelParams(engine=\"hive\", model=\"hive_7b\")\n",
    "hive_70b = ChatModelParams(engine=\"hive\", model=\"hive_70b\")\n",
    "evaluator_llm = gpt_3_5\n",
    "\n",
    "for encoder in [EncoderParams(name=OpenAIModelEnum.embed_small, dim=768)]:\n",
    "    for chunk_hardmax, chunk_softmax, overlap in [(5000, 2000, 500)]:\n",
    "        print(\"Loading knowledge base embeddings...\")\n",
    "        collection_name = \"Courtroom5_NCStatutesPDF\" # f\"{vdb_basename}_{idx}\"\n",
    "        idx += 1\n",
    "        if pymilvus.utility.has_collection(collection_name):\n",
    "            collection = pymilvus.Collection(collection_name)\n",
    "        else:\n",
    "            description = (\n",
    "                f\"Hardmax = {chunk_hardmax}, \"\n",
    "                f\"Softmax = {chunk_softmax}, \"\n",
    "                f\"Overlap = {overlap}, \"\n",
    "                f\"Encoder = {encoder.name}.\"\n",
    "            )\n",
    "            collection = create_collection(\n",
    "                collection_name,\n",
    "                encoder,\n",
    "                description,\n",
    "            )\n",
    "            populate_database(collection, chunk_hardmax, chunk_softmax, overlap, encoder)\n",
    "\n",
    "        # run evaluations with the configured knowledge base\n",
    "        for llm in [gpt_3_5, gpt_4o]:\n",
    "            settings_name = (\n",
    "                f\"chunk-hardmax:{chunk_hardmax}\"\n",
    "                f\"_chunk-softmax:{chunk_softmax}\"\n",
    "                f\"_embeddings:{encoder.name}\"\n",
    "                f\"_dim:{encoder.dim}\"\n",
    "                f\"_reader-model:{llm.model}\"\n",
    "            )\n",
    "            output_file_name = f\"data/NC-court/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            run_rag_tests(\n",
    "                chat_model=llm,\n",
    "                eval_dataset=couples_df,\n",
    "                collection_name=collection_name,\n",
    "                output_file=output_file_name,\n",
    "                verbose=True,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                evaluator_llm,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average evaluation scores as accuracy percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_gpt35 = pd.read_json(\"data/NC-court/rag_chunk-hardmax:5000_chunk-softmax:2000_embeddings:text-embedding-3-small_dim:768_reader-model:gpt-3.5-turbo-0125.json\")[\"eval_score_gpt-3.5-turbo-0125\"]\n",
    "results_gpt4o = pd.read_json(\"data/NC-court/rag_chunk-hardmax:5000_chunk-softmax:2000_embeddings:text-embedding-3-small_dim:768_reader-model:gpt-4o.json\")[\"eval_score_gpt-3.5-turbo-0125\"]\n",
    "#results_hive7b = pd.read_json(\"data/NC-court/employment_eval_hive.json\")[\"eval_score_gpt-4o\"]\n",
    "#results_hive70b = pd.read_json(\"output/employment_eval_hive70b.json\")[\"eval_score_gpt-4o\"]\n",
    "\n",
    "# Define the x-axis labels and values (evaluation models)\n",
    "models = [\"gpt-3.5-turbo-0125\", \"gpt-4o\"]\n",
    "\n",
    "# Define the y-axis values (average scores)\n",
    "y_values = [(results_gpt35.mean() / 5) * 100, (results_gpt4o.mean() / 5) * 100]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(models, y_values, color=[\"lightgreen\", \"green\"])\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Evaluation Accuracy Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average Evaluation Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
