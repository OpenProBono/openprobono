{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenProBono RAG Evaluation\n",
    "\n",
    "This notebook was created using https://huggingface.co/learn/cookbook/en/rag_evaluation as a guide.\n",
    "\n",
    "### 0: Install and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import milvusdb\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Load our knowledge base\n",
    "\n",
    "For this step, we have already loaded the data we wish to evaluate into a `Collection` in our Milvus vector database.\n",
    "\n",
    "#### 1.1: Load sources\n",
    " \n",
    "We have a list of sources we use to filter the documents so we can generate questions about one document at a time. The sources can be files or URLs. We load the list of sources for this example from a file named `urls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"urls\").open() as f:\n",
    "    urls = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Load documents\n",
    "\n",
    "Now that we loaded our source URLs, we can write a function that will get the chunks associated the source from Milvus. We use a boolean expression filter to get the right chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = milvusdb.COURTROOM5\n",
    "\n",
    "def load_url_documents(url: str):\n",
    "    expr = f\"metadata['url']=='{url}'\"\n",
    "    hits = milvusdb.get_expr(collection_name, expr)[\"result\"]\n",
    "    for i in range(len(hits)):\n",
    "        hits[i][\"url\"] = hits[i][\"metadata\"][\"url\"]\n",
    "        del hits[i][\"pk\"]\n",
    "        del hits[i][\"metadata\"]\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Setup question generation LLM\n",
    "\n",
    "We will use `gpt-3.5-turbo` for question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "def call_llm(client: openai.OpenAI, prompt: str):\n",
    "    prompt_msg = {\"role\": \"system\", \"content\": prompt}\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[prompt_msg],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
