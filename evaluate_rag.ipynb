{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenProBono RAG Evaluation\n",
    "\n",
    "This notebook is based on [RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation) by [Aymeric Roucher](https://huggingface.co/m-ric).\n",
    "\n",
    "Any section inside block quotes is a direct quote. The general structure is copied and ideas are paraphrased. Prompts are adjusted to fit OpenProBono's use case. Code has been added and modified.\n",
    "\n",
    ">This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    ">\n",
    ">For an introduction to RAG, you can check [this other cookbook](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain)!\n",
    ">\n",
    ">RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    ">\n",
    "><img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" alt=\"RAG workflow: Knowledge Base, Embedding Model, LLM, LLM Prompt, and more\" height=\"700\"/>\n",
    ">\n",
    ">Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the systemâ€™s performance! So letâ€™s see how to evaluate our RAG system.\n",
    ">\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    ">Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    ">\n",
    ">For our evaluation pipeline, we will need:\n",
    ">\n",
    ">1. An evaluation dataset with question - answer couples (QA couples)\n",
    ">2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    ">\n",
    ">âž¡ï¸ It turns out, we can use LLMs to help us all along the way!\n",
    ">\n",
    ">1. The evaluation dataset will be synthetically generated by an LLM ðŸ¤–, and questions will be filtered out by other LLMs ðŸ¤–\n",
    ">2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ðŸ¤– will then perform the evaluation on this synthetic dataset.\n",
    ">\n",
    ">**Letâ€™s dig into it and start building our evaluation pipeline!**\n",
    "\n",
    "### 0: Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm openai pandas langchain unstructured pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pymilvus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import encoders\n",
    "import milvusdb\n",
    "from models import EncoderParams, OpenAIChatModel, OpenAIEncoder\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Build a synthetic dataset for evaluation\n",
    "\n",
    ">We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
    ">\n",
    ">Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Load sources\n",
    " \n",
    "We have a list of sources we use to generate questions about one source at a time using a generator function. The sources can be files or URLs.\n",
    "\n",
    "Our current sources are the NC General Statutes. We can access them by chapter or by statute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "\n",
    "# for loading by statute\n",
    "root_dir = \"chapter_urls/\"\n",
    "chapter_names = sorted(os.listdir(root_dir))\n",
    "# for loading by chapter\n",
    "with Path(\"urls\").open() as f:\n",
    "    chapter_pdf_urls = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def load_statute_urls(chapter_name: str):\n",
    "    with Path(root_dir + chapter_name).open() as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def generate_statute_elements():\n",
    "    for chapter in chapter_names:\n",
    "        statute_urls = load_statute_urls(chapter)\n",
    "        for statute_url in statute_urls:\n",
    "            elements = partition(statute_url)\n",
    "            yield statute_url, elements\n",
    "\n",
    "def resume_statute_elements(chapter_name: str, statute_url: str):\n",
    "    resume_chapter = next(iter([chapter for chapter in chapter_names if chapter == chapter_name]), None)\n",
    "    resume_chapter_idx = chapter_names.index(resume_chapter) if resume_chapter else 0\n",
    "    for i in range(resume_chapter_idx, len(chapter_names)):\n",
    "        statute_urls = load_statute_urls(chapter_names[i])\n",
    "        resume_statute = next(iter([statute for statute in statute_urls if statute == statute_url]), None)\n",
    "        resume_statute_idx = statute_urls.index(resume_statute) if resume_statute else 0\n",
    "        for j in range(resume_statute_idx, len(statute_urls)):\n",
    "            elements = partition(url=statute_urls[j], content_type=\"application/pdf\")\n",
    "            yield statute_url, elements\n",
    "\n",
    "def generate_chapter_elements():\n",
    "    for chapter_pdf_url in chapter_pdf_urls:\n",
    "        elements = partition(url=chapter_pdf_url, content_type=\"application/pdf\")\n",
    "        yield chapter_pdf_url, elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the sources for question generation by *chunking* them. We will do this again later using different chunking strategies when we store embedded chunks in a vector database for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from unstructured.documents.elements import Element\n",
    "\n",
    "\n",
    "def chunk_elements_qa(elements: list[Element]) -> list[LangchainDocument]:\n",
    "    # unstructured Element -> LangchainDocument for chunking\n",
    "    docs = [\n",
    "        LangchainDocument(\n",
    "            element.text,\n",
    "            metadata = {\n",
    "                \"url\": element.metadata.url,\n",
    "                \"page_number\": element.metadata.page_number,\n",
    "            },\n",
    "        )\n",
    "        for element in elements\n",
    "    ]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2500,\n",
    "        chunk_overlap=250,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "    docs_processed = []\n",
    "    for doc in docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "    return docs_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Setup question generation agents\n",
    "\n",
    "It is necessary to call an LLM for each agent in the eval dataset generation and RAG processes, so let's write a function for them. We use `gpt-3.5-turbo` for the question generation agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = openai.OpenAI()\n",
    "llm_name = OpenAIChatModel.GPT_3_5.value\n",
    "\n",
    "def call_llm(\n",
    "    client: openai.OpenAI,\n",
    "    prompt: str,\n",
    "    model: str = llm_name,\n",
    "    temperature: float = 0.7,\n",
    "    extra_messages: list | None = None,\n",
    "):\n",
    "    if extra_messages is None:\n",
    "        extra_messages = []\n",
    "    prompt_msg = {\"role\": \"system\", \"content\": prompt}\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[prompt_msg, *extra_messages],\n",
    "        max_tokens=1000,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt that is given to our question generation agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cost and time considerations, we generate a number of questions proportional to the length of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_qa_couples(\n",
    "    chunks: list[LangchainDocument],\n",
    "    max_num_questions: int,\n",
    "    min_num_chars: int,\n",
    "    chars_per_page: int,\n",
    "    max_len_answer: int,\n",
    ") -> list:\n",
    "    # filter out chunks with length < min_num_chars\n",
    "    chunks = [chunk for chunk in chunks if len(chunk.page_content) >= min_num_chars]\n",
    "    char_count = sum([len(chunk.page_content) for chunk in chunks])\n",
    "    if char_count < min_num_chars:\n",
    "        return []\n",
    "\n",
    "    question_count = min([max_num_questions, max([1, char_count // chars_per_page])])\n",
    "    print(f\"Generating {question_count} QA couples...\")\n",
    "\n",
    "    couples = []\n",
    "    for sampled_context in tqdm(random.sample(chunks, question_count)):\n",
    "        # Generate QA couple\n",
    "        output_qa_couple = call_llm(\n",
    "            llm_client,\n",
    "            QA_generation_prompt.format(context=sampled_context.page_content),\n",
    "        )\n",
    "        try:\n",
    "            question = output_qa_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].rstrip()\n",
    "            answer = output_qa_couple.split(\"Answer: \")[-1].rstrip()\n",
    "            assert len(answer) < max_len_answer, \"Answer is too long\"\n",
    "            couples.append(\n",
    "                {\n",
    "                    \"context\": sampled_context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": sampled_context.metadata[\"url\"],\n",
    "                },\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_qa_couples(\n",
    "    n: int,\n",
    "    max_questions_per_chunk: int = 4,\n",
    "    min_chars_per_chunk: int = 500,\n",
    "    chars_per_page: int = 2500,\n",
    "    max_len_answer: int = 300,\n",
    "):\n",
    "    if n < max_questions_per_chunk:\n",
    "        # so we generate exactly n couples\n",
    "        max_questions_per_chunk = n\n",
    "    tot_couples = []\n",
    "    for src, elements in generate_chapter_elements():\n",
    "        chunks = chunk_elements_qa(elements)\n",
    "        couples = generate_qa_couples(\n",
    "            chunks,\n",
    "            # so we generate exactly n couples\n",
    "            min([max_questions_per_chunk, n - len(tot_couples)]),\n",
    "            min_chars_per_chunk,\n",
    "            chars_per_page,\n",
    "            max_len_answer,\n",
    "        )\n",
    "        tot_couples += couples\n",
    "        if len(tot_couples) == n:\n",
    "            break\n",
    "    return tot_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTIONS = 3\n",
    "couples = generate_n_qa_couples(NUM_QUESTIONS)\n",
    "display(pd.DataFrame(couples).head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Setup question critique agents\n",
    "\n",
    "The generated questions can be flawed in many ways.\n",
    "\n",
    ">We use an agent to determine if a generated question meets the following criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "\n",
    "- **Groundedness**: can the question be answered from the given context?\n",
    "- **Relevance**: is the question relevant to users? For instance, *\"What are some of Thomas Jefferson's beliefs regarding the rights and liberties of individuals?\"* is not relevant for OpenProBono users.\n",
    "\n",
    ">One last failure case weâ€™ve noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like *\"What is the name of the function used in this guide?\"*. We also build a critique agent for this criteria:\n",
    "\n",
    "- **Standalone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? For instance, *\"What does the term 'legal entity' refer to in this statute?\"* is tailored for a particular statute, but unclear by itself.\n",
    "\n",
    ">We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    ">\n",
    ">ðŸ’¡ ***When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.***\"\n",
    ">\n",
    ">We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to people learning about the legal system.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on the context to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'according to this Article', the rating must be 1.\n",
    "The questions can contain obscure legal definitions or entities like trier of fact or the North Carolina Self-Insurance Security Fund and still be a 5: it must simply be clear to an operator with access to legal documents what the question is about.\n",
    "\n",
    "For instance, the context \"On July 1 of each year, a maximum weekly benefit amount shall be computed.\" and question \"When is the maximum weekly benefit amount computed and adjusted?\" should receive a 1, since the question implicitly mentions the maximum weekly benefit amount, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the context and question.\n",
    "\n",
    "Context: {context}\\n\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_critiques(couples: list[dict]):\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(couples):\n",
    "        evaluations = {\n",
    "            \"groundedness\": call_llm(\n",
    "                llm_client,\n",
    "                question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "            ),\n",
    "            \"relevance\": call_llm(\n",
    "                llm_client,\n",
    "                question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "            ),\n",
    "            \"standalone\": call_llm(\n",
    "                llm_client,\n",
    "                question_standalone_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "            ),\n",
    "        }\n",
    "        try:\n",
    "            for criterion, evaluation in evaluations.items():\n",
    "                score, eval = (\n",
    "                    int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                    evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "                )\n",
    "                output.update(\n",
    "                    {\n",
    "                        f\"{criterion}_score\": score,\n",
    "                        f\"{criterion}_eval\": eval,\n",
    "                    }\n",
    "                )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_questions(\n",
    "        generated_questions: pd.DataFrame,\n",
    "        min_groundedness: int,\n",
    "        min_relevance: int,\n",
    "        min_standalone: int,\n",
    "):\n",
    "    return generated_questions.loc[\n",
    "        (generated_questions[\"groundedness_score\"] >= min_groundedness)\n",
    "        & (generated_questions[\"relevance_score\"] >= min_relevance)\n",
    "        & (generated_questions[\"standalone_score\"] >= min_standalone)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_qa_critiques(couples)\n",
    "couples_df = pd.DataFrame.from_dict(couples)\n",
    "display(couples_df.head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_df = filter_questions(couples_df, 4, 4, 4)\n",
    "display(couples_df.head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "Save the dataset to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"./employment_dataset.json\").open(\"w\") as f:\n",
    "    f.write(couples_df.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples_df = pd.read_json(\"./employment_dataset.json\")\n",
    "display(couples_df.head(NUM_QUESTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Build our RAG System\n",
    "\n",
    "We load the data for RAG into a `Collection` in our Milvus vector database.\n",
    "\n",
    "To accomplish this, we must:\n",
    "\n",
    "1. extract the content from our sources\n",
    "2. chunk the extracted content\n",
    "3. embed the chunks\n",
    "4. insert them to the database\n",
    "\n",
    "We already wrote generator functions to extract the content, `generate_statute_elements` and `generate_chapter_elements`, so our next step is chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Chunk sources into documents\n",
    "\n",
    ">- In this part, **we split the documents from our knowledge base into smaller chunks**: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    ">- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    ">\n",
    ">Many options exist for text splitting:\n",
    ">\n",
    ">- split every n words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    ">- split after n words / character, but only on sentence boundaries\n",
    ">- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    ">\n",
    ">To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    ">\n",
    ">[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "We need a function to chunk the content in a given source into documents. This function should accept a list of items containing extracted text and relevant metadata from the source. In our generator function, `partition` returns a list of `Element` objects. We pass these into a chunking function, `chunk_by_title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "chunking_function = chunk_by_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Embed documents\n",
    "\n",
    ">The retriever acts like an internal search engine: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "An embedding model transforms the documents to vectors, and Milvus creates an index over those vectors for fast and accurate retrieval.\n",
    "\n",
    "We want to evaluate different embedding models, so we represent them with a function that accepts text and returns vectors. In order to easily test different models, the function also accepts an `EncoderParams` object that tells it which embedding model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = encoders.embed_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3: Create vector database\n",
    "\n",
    "The document embeddings are stored in a vector database for fast vector searching during RAG.\n",
    "\n",
    "Below is a function that creates a vector database we can use to store and retrieve the embedded documents. The `name` of the database will be determined by the chosen sources, chunking strategy, and embedding model evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_collection_function = milvusdb.create_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Insert documents\n",
    "\n",
    "The last step is to insert the embedded documents into the newly created vector database.\n",
    "\n",
    "An embedded document is represented by its corresponding `vectors`, `texts`, and `metadatas`. The vector database is given by a Milvus `Collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_BATCH_SIZE = 100\n",
    "\n",
    "def insert_documents(vectors: list, texts: list, metadatas: list, collection: pymilvus.Collection):\n",
    "    data = [vectors, texts, metadatas]\n",
    "    num_chunks = len(vectors)\n",
    "    pks = []\n",
    "    for i in range(0, num_chunks, INSERT_BATCH_SIZE):\n",
    "        batch_vector = data[0][i: i + INSERT_BATCH_SIZE]\n",
    "        batch_text = data[1][i: i + INSERT_BATCH_SIZE]\n",
    "        batch_metadata = data[2][i: i + INSERT_BATCH_SIZE]\n",
    "        batch = [batch_vector, batch_text, batch_metadata]\n",
    "        current_batch_size = len(batch[0])\n",
    "        res = collection.insert(batch)\n",
    "        pks += res.primary_keys\n",
    "        if res.insert_count != current_batch_size:\n",
    "            # the upload failed, try deleting any partially uploaded data\n",
    "            bad_deletes = []\n",
    "            for pk in pks:\n",
    "                delete_res = collection.delete(expr=f\"pk=={pk}\")\n",
    "                if delete_res.delete_count != 1:\n",
    "                    bad_deletes.append(pk)\n",
    "            bad_insert = (\n",
    "                f\"Failure: expected {current_batch_size} insertions but got \"\n",
    "                f\"{res.insert_count}. \"\n",
    "            )\n",
    "            if bad_deletes:\n",
    "                bad_insert += f\"Dangling data: {','.join(bad_deletes)}\"\n",
    "            else:\n",
    "                bad_insert += \"Any partially uploaded data has been deleted.\"\n",
    "            return {\"message\": bad_insert}\n",
    "    return {\"message\": \"Success\", \"num_chunks\": num_chunks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Populate the vector database\n",
    "\n",
    "So far, the parameters we can control are:\n",
    "\n",
    "1. Chunking strategy parameters\n",
    "    - chunk hardmax: the maximum number of characters in a chunk\n",
    "    - chunk softmax: the preferred maximum number of characters in a chunk (see `new_after_n_chars` in `chunk_by_title` for more)\n",
    "    - overlap: the number of characters to overlap between consecutive chunks\n",
    "2. Embedding model\n",
    "\n",
    "There are also some parameters we can't currently control:\n",
    "\n",
    "1. Document loader (unstructured)\n",
    "2. Question generation chunking strategy (LangChain's `RecursiveCharacterTextSplitter`)\n",
    "2. Base RAG chunking strategy (unstructured's `chunk_by_title`)\n",
    "3. Vector Index (Zilliz autoindex)\n",
    "\n",
    "Once we decide on values for the parameters we can control, we can chunk and embed our sources into a vector database. We can then use the database to generate a synthetic dataset of question-answer pairs and/or evaluate agents' responses to a generated (or loaded) dataset of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_database(collection: pymilvus.Collection, chunk_hardmax: int, chunk_softmax: int, overlap: int, encoder: EncoderParams):\n",
    "    for src, elements in generate_chapter_elements():\n",
    "        chunks = chunking_function(\n",
    "            elements,\n",
    "            max_characters=chunk_hardmax,\n",
    "            new_after_n_chars=chunk_softmax,\n",
    "            overlap=overlap,\n",
    "        )\n",
    "        num_chunks = len(chunks)\n",
    "        texts, metadatas = [], []\n",
    "        for i in range(num_chunks):\n",
    "            texts.append(chunks[i].text)\n",
    "            metadatas.append(chunks[i].metadata.to_dict())\n",
    "        vectors = embedding_function(texts, encoder)\n",
    "        result = insert_documents(vectors, texts, metadatas, collection)\n",
    "        if result[\"message\"] != \"Success\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to rerun this notebook with data that has already been inserted to Milvus. If so, we need different generator functions to load the data from Milvus rather than the source URLs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import ElementMetadata, Text\n",
    "\n",
    "\n",
    "def vdb_source_documents(collection_name: str, source: str):\n",
    "    expr = f\"metadata['url']=='{source}'\"\n",
    "    hits = milvusdb.get_expr(collection_name, expr)[\"result\"]\n",
    "    for i in range(len(hits)):\n",
    "        hits[i][\"url\"] = hits[i][\"metadata\"][\"url\"]\n",
    "        hits[i][\"page_number\"] = hits[i][\"metadata\"][\"page_number\"]\n",
    "        del hits[i][\"pk\"]\n",
    "        del hits[i][\"metadata\"]\n",
    "    return [\n",
    "        Text(\n",
    "            text=hit[\"text\"],\n",
    "            metadata=ElementMetadata(\n",
    "                url=source,\n",
    "                page_number=hit[\"page_number\"],\n",
    "            ),\n",
    "        ) for hit in hits\n",
    "    ]\n",
    "\n",
    "def load_statute_elements(collection_name: str):\n",
    "    for chapter in chapter_names:\n",
    "        statute_urls = load_statute_urls(chapter)\n",
    "        for statute_url in statute_urls:\n",
    "            yield vdb_source_documents(collection_name, statute_url)\n",
    "\n",
    "def load_chapter_elements(collection_name: str):\n",
    "    for chapter_pdf_url in chapter_pdf_urls:\n",
    "        yield vdb_source_documents(collection_name, chapter_pdf_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Reader - LLM ðŸ’¬\n",
    "\n",
    ">In this part, the LLM Reader reads the retrieved documents to formulate its answer.\n",
    "\n",
    "OpenProBono offers various LLM Readers using models offered by OpenAI, HuggingFace, and more. We will create a simple LLM Reader connected to the OpenAI API for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT_UNFORMATTED = (\n",
    "    \"Using the information contained in the context, \"\n",
    "    \"give a comprehensive answer to the question. \"\n",
    "    \"Respond only to the question asked, response should be concise and relevant to the question. \"\n",
    "    \"Provide the number of the source document when relevant. \"\n",
    "    \"If the answer cannot be deduced from the context, do not give an answer.\"\n",
    ")\n",
    "\n",
    "VANILLA_PROMPT = (\n",
    "    \"Give a comprehensive answer to the question. \"\n",
    "    \"Respond only to the question asked, response should be concise and relevant to the question. \"\n",
    "    \"If the answer cannot be deduced, do not give an answer.\"\n",
    "    \"\\n\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    collection_name: str,\n",
    "    question: str,\n",
    "    k: int = 7,\n",
    ") -> tuple[str, list[dict]]:\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = milvusdb.query(collection_name, question, k)\n",
    "    relevant_docs = [doc[\"entity\"][\"text\"] for doc in relevant_docs[\"result\"]]  # keep only the text\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {i!s}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = call_llm(llm_client, final_prompt, temperature=0)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Benchmarking the RAG System\n",
    "\n",
    ">The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    ">\n",
    ">To this end, **we setup a judge agent**. âš–ï¸ðŸ¤–\n",
    ">\n",
    ">Out of the different RAG evaluation metrics, we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    ">ðŸ’¡ *In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in Prometheus's prompt template: this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples.*\n",
    ">\n",
    ">ðŸ’¡ *Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_tests(\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    collection_name: str,\n",
    "    output_file: str,\n",
    "    verbose: bool | None = True,\n",
    "    test_settings: str = \"\",  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Run RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with Path(output_file).open() as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for _, example in tqdm(eval_dataset.iterrows(), total=len(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(collection_name, question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": list(relevant_docs),\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with Path(output_file).open(\"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MSG = \"You are a fair evaluator language model.\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_name = llm_name\n",
    "eval_temperature = 0\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    evaluator_name: str,\n",
    ") -> None:\n",
    "    \"\"\"Evaluate generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if Path(answer_path).is_file():  # load previous generations if they exist\n",
    "        with Path(answer_path).open() as f:\n",
    "            answers = json.load(f)\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = EVALUATION_PROMPT.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_msg = {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        eval_result = call_llm(llm_client, EVALUATION_SYSTEM_MSG, evaluator_name, eval_temperature, [eval_msg])\n",
    "        feedback, score = (item.strip() for item in eval_result.split(\"[RESULT]\"))\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with Path(answer_path).open(mode=\"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ðŸš€ Let's run the tests and evaluate answers!ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import UTC, datetime\n",
    "\n",
    "if not Path(\"./output\").exists():\n",
    "    Path.mkdir(\"./output\")\n",
    "\n",
    "vdb_basename = \"EmploymentEval_\" + datetime.now(UTC).strftime(\"%Y%m%d\")\n",
    "idx = 1\n",
    "for chunk_hardmax, chunk_softmax, overlap in [(2500, 1000, 250)]:\n",
    "    for encoder in [\n",
    "        EncoderParams(name=OpenAIEncoder.SMALL.value, dim=768),\n",
    "    ]:\n",
    "        settings_name = (\n",
    "            f\"chunk-hardmax:{chunk_hardmax}\"\n",
    "            f\"_chunk-softmax:{chunk_softmax}\"\n",
    "            f\"_embeddings:{encoder.name}\"\n",
    "            f\"_dim:{encoder.dim}\"\n",
    "            f\"_reader-model:{llm_name}\"\n",
    "        )\n",
    "        output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "        print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "        print(\"Loading knowledge base embeddings...\")\n",
    "        collection_name = f\"{vdb_basename}_{idx}\"\n",
    "        idx += 1\n",
    "        if pymilvus.utility.has_collection(collection_name):\n",
    "            collection = pymilvus.Collection(collection_name)\n",
    "        else:\n",
    "            description = (\n",
    "                f\"Hardmax = {chunk_hardmax}, \"\n",
    "                f\"Softmax = {chunk_softmax}, \"\n",
    "                f\"Overlap = {overlap}, \"\n",
    "                f\"Encoder = {encoder.name}.\"\n",
    "            )\n",
    "            collection = create_collection_function(\n",
    "                collection_name,\n",
    "                encoder,\n",
    "                description,\n",
    "            )\n",
    "            populate_database(collection, chunk_hardmax, chunk_softmax, overlap, encoder)\n",
    "\n",
    "        print(\"Running RAG...\")\n",
    "        run_rag_tests(\n",
    "            eval_dataset=couples_df,\n",
    "            collection_name=collection_name,\n",
    "            output_file=output_file_name,\n",
    "            verbose=True,\n",
    "            test_settings=settings_name,\n",
    "        )\n",
    "\n",
    "        print(\"Running evaluation...\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            evaluator_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average evaluation scores as accuracy percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_gpt = pd.read_json(\"output/employment_eval_gpt35.json\")[\"eval_score_gpt-3.5-turbo-0125\"]\n",
    "results_hive7b = pd.read_json(\"output/employment_eval_hive.json\")[\"eval_score_gpt-3.5-turbo-0125\"]\n",
    "results_hive70b = pd.read_json(\"output/employment_eval_hive70b.json\")[\"eval_score_gpt-3.5-turbo-0125\"]\n",
    "\n",
    "# Define the x-axis labels and values (evaluation models)\n",
    "models = [\"gpt-3.5-turbo-0125\", \"hive llm chat 7B\", \"hive llm chat 70B\"]\n",
    "\n",
    "# Define the y-axis values (average scores)\n",
    "y_values = [(results_gpt.mean() / 5) * 100, (results_hive7b.mean() / 5) * 100, (results_hive70b.mean() / 5) * 100]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(models, y_values, color=[\"lightgreen\", \"lightblue\", \"blue\"])\n",
    "\n",
    "# Add values above each bar\n",
    "for i, value in enumerate(y_values):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha=\"center\")\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Evaluation Accuracy Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average Evaluation Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# for storing chunks\n",
    "def make_jsonl():\n",
    "    with Path(\"data/ncstatutes.jsonl\").open(\"w\") as f:\n",
    "        for pdf_fname in tqdm(os.listdir(\"data/pdfs/\")):\n",
    "            if not pdf_fname.endswith(\".pdf\"):\n",
    "                continue\n",
    "            elements = partition(filename=\"data/pdfs/\" + pdf_fname)\n",
    "            chunks = chunk_by_title(elements, max_characters=2500, combine_text_under_n_chars=500, new_after_n_chars=1000, overlap=250)\n",
    "            for chunk in chunks:\n",
    "                json.dump({\"text\":chunk.text, \"metadata\":{\"source\":chunk.metadata.filename, \"page_number\":chunk.metadata.page_number}}, f)\n",
    "                f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
